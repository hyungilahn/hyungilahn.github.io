<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Search | Hyungil Ahn</title>
	<meta name="description" content="deep probabilistic programming, bayesian deep learning">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<!-- CSS -->
	<link rel="stylesheet" href="/assets/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="/search.html">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Hyungil Ahn" href="/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css">
	<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<h1 class="site-title">
			<a href="/">Hyungil Ahn</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
			
			
			
			
			<li>
				<a class="page-link" href="/about/">
					About
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<li>
				<a class="page-link" href="/tags.html">
					tags
				</a>
			</li>
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled  -->
			


<li>
	<a href="mailto:hyungil@gmail.com" title="Email">
		<i class="fa fa-fw fa-envelope"></i>
	</a>
</li>



















<li>
	<a href="https://www.linkedin.com/in/hyung-il-ahn/" title="Follow on LinkedIn">
		<i class="fa fa-fw fa-linkedin"></i>
	</a>
</li>






















            
            <!-- Search bar -->
            
		</ul>
	</nav>
    
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/')">
    <h1 class="title">Search</h1>
    
  </header>
  <section class="post-content"><div class="search">
    <div id="search-results"></div>
    <p id="not-found" style="display: none">
        No results found.
    </p>
</div>


<script>
  window.store = {
    
      "2018-01-15-deep-prob-modeling-html": {
        "title": "Deep Probabilistic Programming Combines Bayesian Inference and Deep Learning",
        "tags": "Deep_Probabilistic_Programming",
        "date": "January 15, 2018",
        "author": "",
        "category": "",
        "content": "Deep probabilistic programming is a method of implementing “Bayesian” probabilistic modeling on “differentiable” deep learning frameworks. This provides a style of language to define complex (composite, hierarchical) models with multiple components and incorporate probabilistic uncertainty about latent variables or model parameters into predictions.Deep probabilistic programming can be characterized by approximate Bayesian inference 1 (calculating the approximate posterior probability distribution of latent variables or model parameters by incorporating the information from the observed data) on differentiable programming (parametric optimization by deep learning). Thus, I think that deep probabilistic programming can be also called “Bayesian differentiable programming”.Uber’s Pyro framework emphasizes the following four characteristics of deep probabilistic programming.      Why probabilistic modeling? To correctly capture uncertainty in models and predictions for unsupervised and semi-supervised learning, and to provide AI systems with declarative prior knowledge.    Why (universal) probabilistic programs? To provide a clear and high-level, but complete, language for specifying complex models.    Why deep probabilistic models? To learn generative knowledge from data and reify knowledge of how to do inference.    Why inference by optimization? To enable scaling to large data and leverage advances in modern optimization and variational inference.  Why Deep and Differentiable Programming?Deep learning frameworks (e.g., PyTorch, TensorFlow, MxNet) enable defining a target model in a deep and composite network structure assembling the building blocks of component models (= parametric linear or non-linear functions including neural nets) that run in data-dependent, procedural and conditional manner. Each component may be based on different sets of feature variables. Also, they provide a tool to estimate the model parameters in terms of differentiable optimization like stochastic gradient decent (SGD) and back-propagation algorithms.Yann LeCun earlier made good points on differentiable programming.  “Differentiable Programming is little more than a rebranding of the modern collection Deep Learning techniques, the same way Deep Learning was a rebranding of the modern incarnations of neural nets with more than two layers. But the important point is that people are now building a new kind of software by assembling networks of parameterized functional blocks and by training them from examples using some form of gradient-based optimization. An increasingly large number of people are defining the network procedurally in a data-dependant way (with loops and conditionals), allowing them to change dynamically as a function of the input data fed to them. It’s really very much like a regular progam, except it’s parameterized, automatically differentiated, and trainable/optimizable. Dynamic networks have become increasingly popular (particularly for NLP), thanks to deep learning frameworks that can handle them such as PyTorch and Chainer (note: our old deep learning framework Lush could handle a particular kind of dynamic nets called Graph Transformer Networks, back in 1994. It was needed for text recognition).”Since Bayesian modeling is based on a probabilistic model of the generative process relating the observed data with the uncertain latent variables (= generating parameters), it is very desirable to have the representational power of a deep and composite network model to sufficiently describe the potentially complex generative processes with multiple input variables. In addition, the exact calculation of the posterior distribution of latent variables requires doing the integral calculation to obtain the evidence of the observed data with the assumed prior distribtuion, so this is intractable in most problems. Thus, we need approximate Bayesian inference techniques, such as variational inference (VI). Thankfully, VI transforms the approximate posterior inference problems into the optimization problems searching for the best hyperparameters of approximate posterior distribution (often assumed to be Gaussian distributions). We will discuss it in detail below.Why Bayesian Inference?Bayesian probabilistic modeling provides a unified scheme on how to update the uncertain information (or infer the posterior distributions) about modeling parameters or latent variables using observed data. It also assumes the specification of generative processes (or model functions describing how outputs are produced from inputs) and prior distributions of modeling parameters or latent variables. This specification allows for easy incorporation of prior knowledge into the model form and associated parameter uncertainty.Although the initial choice of compared models and associated prior distributions may depend on our domain knowledge about the underlying problems, bayesian reasoning provides an objective scheme to compare different models and priors.Bayesian modeling allows us to build more robust and less overfitted models under uncertainty and predict probabilistic estimates about target variables in the model. By incorporating the known form of a physics-based equation describing the potental causal relationships of variables in the underlying phenomenon into our modeling and set the priors on the model parameters, we can build a heuristically reasonable, more generalizable and updatable model with insufficient data.It sounds all good and simple, but a key difficulty in Bayesian probabilistic modeling arises from calculating the posterior distributions for a given complicated model structure and prior. It is very often intractable to compute the “exact” posterior distribution, but the variational inference (VI), one of the important methods in deep probabilistic programming, present a commonly-applicable approach to compute the “approximate” bayesian posterior.Since VI transforms Bayesian posterior inference problems (i.e., learning uncertain modeling parameters or latent variables) into optimization problems, the SGD optimization in underlying deep learning frameworks can solve the posterior inference problems.Deep Probabilistic Programming = Bayesian Differentiable ProgrammingCompared to non-probabilistic deep learning frameworks 2 that aim to make “deterministic” models for point-estimate predictions, deep probabilistic programming frameworks enables us to 1) specify “probabilistic” models involving the uncertain distributions of model parameters or latent variables, 2) provide approximate Bayesian inferences (e.g., variational inferences) using the powerful stochastic gradient decent algorithms of the original deep learning. Thus, deep probabilistic programming naturally integrates the benefits of bayesian modeling and deep learning.Application: Bayesian Regression with a Parametric Function (e.g., knowledge-based known function form, NNs)In Bayesian modeling we posit that the model parameters (a.k.a. latent or hidden variables) generating the observed data are uncertain in our knowledge. Thus, our information about the true values of generating variables is described by a probability. That is, we use a probability to denote our uncertainty about the hidden variables selected to describe the generating process.Suppose we have a dataset  where each data point has feature input vector  and observed output variable  ​The goal of Bayesian regression is to fit a function to the data:  assuming that  is the uncertain latent variables described by a probability distribution.There are important modeling assumptions here.   is an assumed generating function we specify with unexplained error .   is a deterministic function for any sampled value of . The function  may be any known form of an equation or a neural network involving the model parameters .  The level of  is assumed to be fixed as a constant value and also related to how accurately we may specify our function .Whereas a non-Bayesian (deterministic) approach views  as a fixed variable to be estimated, a Bayesian (probabilistic) approach regards  as an uncertain variable whose probability distribution is to be estimated to explain the observed data. Maximum likelihood (ML) or maximum a posteriori (MAP) estimations are well-known non-Bayesian approaches determining a fixed .Now let’s represent the above Bayesian regression in terms of probability distributions.In the Bayesian perspective the complete generative process should be always described in the joint probability distribution of all observed and latent variables.Since  is given and  and  are known and  fixed, the joint distribution for the complete generative process is  where  denotes the hypothesis space of model form .Factorizing , we represent the complete generative process in the combination of the likelihood and the prior distributions. It is important to note that the exact forms of the likelihood and the prior distributions are part of our modeling assumptions.  The likelihood  is our assumed probability model to describe a generating process of the observed variable  from a sample of latent variables . Assuming that the likelihood is normally distributed with  as  (= the expected value of ) and  as the Gaussian noise level,Note that a known deterministic physical model  can be easily incorporated into the likelihood.  The prior  is our assumed probability model to represent the uncertain information of latent variables  (= model parameters) before we consider the observed data ..Bayesian posterior inference is to update our probabilistic information about  after observing the data  and considering the likelihood.  Mathematically the Bayes’ rule (a.k.a. inverse probability) provides this update rule and the posterior distribution representing the uncertainty about  is computed as = .The evidence  is the normalizing constant of the posterior distribution and should be calculated to obtain the exact posterior distribution 3. It is the probability that the model observes the data with the assumed likelihood and the prior distributions. Note that it is also the marginalized probability of likelihood  prior over the uncertain parameters .Since this is intractable to be exactly calculated, we involve an approximate Bayesian inference such as variational inference (VI) or Markov chain Monte Carlo (MCMC).Importantly, the Bayesian prediction of  for a new  is essentially the marginalization of likelihood  posterior over the uncertain parameters , incorporating the posterior uncertainty about parameters  into predictions.Variational InferenceFor simpler notations, we define the generating parameters . Note that  posits the likelihood variance (or unexplained error level) and  posits the prior variance (or model parameter uncertainty).  Now we can denotethe prior  by  and the posteror  by .The prior distribution: The posterior distribution:  = The aim of variational inference is to approximate the posterior  by a simpler probability distribution  where  is called the variational parameters.Therefore,Note that the log evidence  is lower-bounded because  is always greater than 0. The lower bound  is called the Evidence Lower Bound (ELBO) or the variational free energy. Since  is fixed for a given  (prior variance  and likelihood variance ) and the  hypothesis space  of the model function form , the maximization of ELBO by adjusting the variational parameters  (or ) minimizes , which makes the approximate posterior distribution  closer to the true posterior distribution . Also, note that when ELBO is maximized up to the log evidence ,  becomes 0, making the approxmiate posterior  the same as the exact posterior .The existence of ELBO transforms the posterior inference problem into the optmization problem, which can be solved using the minus ELBO as the loss function of SGD algorithm in DL frameworks. In addition, ELBO can be represented in a different way to clarify how the approximate posterior distribution of parameters  shaped by  reckons the balance between the likelihood and the prior.The first term  motivates the distribution  to concentrate on the  values with which the model have higher likelihood , whereas the second term the minus  inspires the approximate prior  to be less deviated from the given prior distribution .Prediction Errors as Probabilistic InterpretationsIn Bayesian approach we make our assumptions on both likelihood and prior distributions. The likelihood distribution posits the model function form  and the unexplained error level . The prior distribution has an assumed variance .Importantly, the Bayesian prediction of  for a new  is essentially the marginalization of likelihood  posterior over the uncertain parameters , incorporating the posterior uncertainty about parameters  into predictions.First, Note that the unexplained error  =  =  = , since  where  is the hidden true value (before a noise is being added) and  is the inherent noise. Also,  is the misspecified model bias error,  and .Thus, the likelihood for a given  is shaped by both the misspecified model bias error and the inherent error. As the ratio of the ratio  goes up, the likelihood goes down. In addition, the normalizing factor  decreases the likelihood with higher .Second, the term  where  put different weights over  values. That is,  closer to the current prior mean  is more weighted, allowing for the tendency toward a smaller deviation from the current prior. In particular, when ,  corresponds to a regularization term preferring small values of  decreasing the tendency of overfitting and higher model parameter uncertainty (= model variance error).In general, when the set of features used in modeling are:  only the essential features          we tend to obtain a generalizable model with a good fit        all essential features + many irrelevant features          too complicated model (overfit) and high model variance (model parameter uncertainty)        insufficient essential features          too simple model (underfit) and high model bias (model misspecification)        insufficient essential features + many irrelevant features          non-generalizable model with a bad fit              References[1] https://eng.uber.com/pyro/[2]  https://www.facebook.com/yann.lecun/posts/10155003011462143[3] David MacKay. Information Theory, Inference, and Learning Algorithms.  http://www.inference.org.uk/mackay/Book.html            Learning the probability distributions of model parameters or latent variables is called inference in probabilistic modeling. Bayesian inference often involves approximating posterior distributions, since the exact calculations are often intractable. In parameterized models, variational inference (VI) approximate the posterior distributions through optimization (differential programming). &#8617;              Non-probabilistic deep learning models do not assume the uncertainty in model parameters (e.g., weights in NN). &#8617;              Compared to the Bayesian approach requiring the estimation of the evidence and the posterior distribution, the maximum a posteriori (MAP) estimation simply considers the point parameter estimate  maximizing the numerator part of the posterior (= Likelihood  Prior).  &#8617;      ",
        "url": "//2018/01/15/deep-prob-modeling.html"
      }
      ,
    
      "2018-01-10-temp-html": {
        "title": "Deep and Hierarchical Implicit Models",
        "tags": "",
        "date": "January 10, 2018",
        "author": "",
        "category": "",
        "content": "I’m excited to announce a paper that Rajesh Ranganath, Dave Blei, andI released today on arXiv, titledDeep and Hierarchical Implicit Models.Implicit probabilistic models are all about sampling as a primitive:they define a process to simulate data and do not require tractabledensities((Diggle &amp; Gratton, 1984),(Hartig, Calabrese, Reineking, Wiegand, &amp; Huth, 2011)). We leverage this fundamental idea to develop new classes ofmodels: they encompass simulators in the scientific communities,generative adversarial networks(Goodfellow et al., 2014),and deep generative models such as sigmoidbelief nets(Neal, 1990)and deep latent Gaussian models((Rezende, Mohamed, &amp; Wierstra, 2014),(Kingma &amp; Welling, 2014)).These modeling developments could not really be done withoutinference, and we develop a variational inference algorithm thatunderpins them all.Biased as I am, I think this is quite a dense paper—chock full ofsimple ideas that are rife with deep implications. There are manynuggets of wisdom that I could ramble on about, and I just might inseparate blog posts.As a practical example, we show how you can take any standard neuralnetwork and turn it into a deep implicit model: simply inject noiseinto the hidden layers. The hidden units in these layers are nowinterpreted as latent variables. Further, the induced latent variablesare astonishingly flexible, going beyond Gaussians (or exponentialfamilies(Ranganath, Tang, Charlin, &amp; Blei, 2015))to arbitrary probability distributions. Deep generative modeling couldnot be any simpler!Here’s a 2-layer deep implicit model in Edward.It defines the generative process,This generates layers of latent variables ,  and data  via functions of noise .import tensorflow as tffrom edward.models import Normalfrom keras.layers import DenseN = 55000  # number of data pointsd = 100  # noise dimensionality# random noise is Normal(0, 1)eps2 = Normal(tf.zeros([N, d]), tf.ones([N, d]))eps1 = Normal(tf.zeros([N, d]), tf.ones([N, d]))eps0 = Normal(tf.zeros([N, d]), tf.ones([N, d]))# alternate latent layers z with hidden layers hz2 = Dense(128, activation='relu')(eps2)h2 = Dense(128, activation='relu')(z2)z1 = Dense(128, activation='relu')(tf.concat([eps1, h2], 1))h1 = Dense(128, activation='relu')(z1)x  = Dense(10, activation=None)(tf.concat([eps0, h1], 1))The model uses Keras, where Dense(256)(x) denotes a fully connectedlayer with  hidden units applied to input x. To define astochastic layer, we concatenate noise with the previous layer. Themodel alternates between stochastic and deterministic layers togenerate data points .Check out the paper for how you can work with, or even interpret, such a model.EDIT (2017/03/02): The algorithm is now merged into Edward.ReferencesDiggle, P. J., &amp; Gratton, R. J. (1984). Monte Carlo methods of inference for implicit statistical models. Journal of the Royal Statistical Society Series B.Hartig, F., Calabrese, J. M., Reineking, B., Wiegand, T., &amp; Huth, A. (2011). Statistical inference for stochastic simulation models - theory and application. Ecology Letters, 14(8), 816–827.Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative Adversarial Nets. In Neural Information Processing Systems.Neal, R. M. (1990). Learning Stochastic Feedforward Networks.Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In International Conference on Machine Learning.Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. In International Conference on Learning Representations.Ranganath, R., Tang, L., Charlin, L., &amp; Blei, D. M. (2015). Deep Exponential Families. In Artificial Intelligence and Statistics.",
        "url": "//2018/01/10/temp.html"
      }
      ,
    
      "2018-01-05-random-thoughts-html": {
        "title": "Random Thoughts",
        "tags": "",
        "date": "January 5, 2018",
        "author": "",
        "category": "",
        "content": "We should consider the deep learning framework as new style of programming",
        "url": "//2018/01/05/random-thoughts.html"
      }
      ,
    
      "2014-11-30-sample-post-html": {
        "title": "Sample post",
        "tags": "test, sample",
        "date": "November 30, 2014",
        "author": "",
        "category": "",
        "content": "Consectetur adipiscing elit. Donec a diam lectus. Sed sit amet ipsum mauris. Maecenas congue ligula ac quam viverra nec consectetur ante hendrerit. Donec et mollis dolor. Praesent et diam eget libero egestas mattis sit amet vitae augue. Nam tincidunt congue enim, ut porta lorem lacinia consectetur. Donec ut libero sed arcu vehicula ultricies a non tortor. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean ut gravida lorem.  Consectetur adipiscing elit  Donec a diam lectus  Sed sit amet ipsum maurisUt turpis felis, pulvinar a semper sed, adipiscing id dolor. Pellentesque auctor nisi id magna consequat sagittis. Curabitur dapibus enim sit amet elit pharetra tincidunt feugiat nisl imperdiet. Ut convallis libero in urna ultrices accumsan. Donec sed odio eros. Donec viverra mi quis quam pulvinar at malesuada arcu rhoncus. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. In rutrum accumsan ultricies. Mauris vitae nisi at sem facilisis semper ac in est.Nunc diam velit, adipiscing ut tristique vitae, sagittis vel odio. Maecenas convallis ullamcorper ultricies. Curabitur ornare, ligula semper consectetur sagittis, nisi diam iaculis velit, id fringilla sem nunc vel mi. Nam dictum, odio nec pretium volutpat, arcu ante placerat erat, non tristique elit urna et turpis. Quisque mi metus, ornare sit amet fermentum et, tincidunt et orci. Fusce eget orci a orci congue vestibulum.Ut dolor diam, elementum et vestibulum eu, porttitor vel elit. Curabitur venenatis pulvinar tellus gravida ornare. Sed et erat faucibus nunc euismod ultricies ut id justo. Nullam cursus suscipit nisi, et ultrices justo sodales nec. Fusce venenatis facilisis lectus ac semper. Aliquam at massa ipsum. Quisque bibendum purus convallis nulla ultrices ultricies. Nullam aliquam, mi eu aliquam tincidunt, purus velit laoreet tortor, viverra pretium nisi quam vitae mi. Fusce vel volutpat elit. Nam sagittis nisi dui.  Suspendisse lectus leo, consectetur in tempor sit amet, placerat quis nequeEtiam luctus porttitor lorem, sed suscipit est rutrum non. Curabitur lobortis nisl a enim congue semper. Aenean commodo ultrices imperdiet. Vestibulum ut justo vel sapien venenatis tincidunt.Phasellus eget dolor sit amet ipsum dapibus condimentum vitae quis lectus. Aliquam ut massa in turpis dapibus convallis. Praesent elit lacus, vestibulum at malesuada et, ornare et est. Ut augue nunc, sodales ut euismod non, adipiscing vitae orci. Mauris ut placerat justo. Mauris in ultricies enim. Quisque nec est eleifend nulla ultrices egestas quis ut quam. Donec sollicitudin lectus a mauris pulvinar id aliquam urna cursus. Cras quis ligula sem, vel elementum mi. Phasellus non ullamcorper urna.",
        "url": "//2014/11/30/sample-post.html"
      }
      ,
    
      "2014-11-29-feature-images-html": {
        "title": "Feature images",
        "tags": "",
        "date": "November 29, 2014",
        "author": "",
        "category": "",
        "content": "This is an example of a post which includes a feature image specified in the front matter of the post. The feature image spans the full-width of the page, and is shown with the title on permalink pages.",
        "url": "//2014/11/29/feature-images.html"
      }
      ,
    
      "2014-11-28-markdown-and-html-html": {
        "title": "Markdown and HTML",
        "tags": "",
        "date": "November 28, 2014",
        "author": "",
        "category": "",
        "content": "Jekyll supports the use of Markdown with inline HTML tags which makes it easier to quickly write posts with Jekyll, without having to worry too much about text formatting. A sample of the formatting follows.Tables have also been extended from Markdown:            First Header      Second Header                  Content Cell      Content Cell              Content Cell      Content Cell      Here’s an example of an image, which is included using Markdown:Highlighting for code in Jekyll is done using Pygments or Rouge. This theme makes use of Rouge by default.// count to tenfor (var i = 1; i &lt;= 10; i++) {    console.log(i);}// count to twentyvar j = 0;while (j &lt; 20) {    j++;    console.log(j);}Type Theme uses KaTeX to display maths. Equations such as  can be displayed inline.Alternatively, they can be shown on a new line:",
        "url": "//2014/11/28/markdown-and-html.html"
      }
      ,
    
      "2014-01-02-introducing-lanyon-html": {
        "title": "Introducing Lanyon",
        "tags": "",
        "date": "January 2, 2014",
        "author": "",
        "category": "",
        "content": "Lanyon is an unassuming Jekyll theme that places content first by tucking away navigation in a hidden drawer. It’s based on Poole, the Jekyll butler.Built on PoolePoole is the Jekyll Butler, serving as an upstanding and effective foundation for Jekyll themes by @mdo. Poole, and every theme built on it (like Lanyon here) includes the following:  Complete Jekyll setup included (layouts, config, 404, RSS feed, posts, and example page)  Mobile friendly design and development  Easily scalable text and component sizing with rem units in the CSS  Support for a wide gamut of HTML elements  Related posts (time-based, because Jekyll) below each post  Syntax highlighting, courtesy Pygments (the Python-based code snippet highlighter)Lanyon featuresIn addition to the features of Poole, Lanyon adds the following:  Toggleable sliding sidebar (built with only CSS) via ☰ link in top corner  Sidebar includes support for textual modules and a dynamically generated navigation with active link support  Two orientations for content and sidebar, default (left sidebar) and reverse (right sidebar), available via &lt;body&gt; classes  Eight optional color schemes, available via &lt;body&gt; classesHead to the readme to learn more.Browser supportLanyon is by preference a forward-thinking project. In addition to the latest versions of Chrome, Safari (mobile and desktop), and Firefox, it is only compatible with Internet Explorer 9 and above.DownloadLanyon is developed on and hosted with GitHub. Head to the GitHub repository for downloads, bug reports, and features requests.Thanks!",
        "url": "//2014/01/02/introducing-lanyon.html"
      }
      
    
  };
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/0.7.1/lunr.min.js"></script>
<script src="/assets/js/search.js"></script>
</section>
</article>

    </div>
    
<script src="/assets/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text"></p>
</footer>


  </body>
</html>
