<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Search | Hyungil Ahn</title>
	<meta name="description" content="A website with blog posts and pages">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<!-- CSS -->
	<link rel="stylesheet" href="/assets/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="/search.html">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="Hyungil Ahn" href="/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css">
	<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js"></script>
	

	<!-- Google Analytics -->
	
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/">
			<img class="avatar" src="/assets/img/avatar.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/">Hyungil Ahn</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
			
			
			
			
			<li>
				<a class="page-link" href="/about/">
					About
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			<li>
				<a class="page-link" href="/tags.html">
					tags
				</a>
			</li>
			
			
			<!-- Social icons from Font Awesome, if enabled  -->
			




















<li>
	<a href="https://www.linkedin.com/in/hyung-il-ahn/" title="Follow on LinkedIn">
		<i class="fa fa-fw fa-linkedin"></i>
	</a>
</li>






















            
            <!-- Search bar -->
            
		</ul>
	</nav>
    
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/')">
    <h1 class="title">Search</h1>
    
  </header>
  <section class="post-content"><div class="search">
    <div id="search-results"></div>
    <p id="not-found" style="display: none">
        No results found.
    </p>
</div>


<script>
  window.store = {
    
      "2018-01-15-deep-prob-modeling-html": {
        "title": "Deep Probabilistic Programming Combines Bayesian Inference and Deep Learning",
        "tags": "Deep_Probabilistic_Programming",
        "date": "January 15, 2018",
        "author": "",
        "category": "",
        "content": "Deep probabilistic programming is a method of implementing “Bayesian” probabilistic modeling on “differentiable” deep learning frameworks. This provides a style of language to define complex (composite, hierarchical) models with multiple components and incorporate probabilistic uncertainty about latent variables or model parameters into predictions.Deep probabilistic programming can be characterized by approximate Bayesian inference 1 (calculating the approximate posterior probability distribution of latent variables or model parameters by incorporating the information from the observed data) on differentiable programming (parametric optimization by deep learning). Thus, I think that deep probabilistic programming can be also called “Bayesian differentiable programming”.Uber’s Pyro framework emphasizes the following four characteristics of deep probabilistic programming.      Why probabilistic modeling? To correctly capture uncertainty in models and predictions for unsupervised and semi-supervised learning, and to provide AI systems with declarative prior knowledge.    Why (universal) probabilistic programs? To provide a clear and high-level, but complete, language for specifying complex models.    Why deep probabilistic models? To learn generative knowledge from data and reify knowledge of how to do inference.    Why inference by optimization? To enable scaling to large data and leverage advances in modern optimization and variational inference.  Why Deep and Differentiable Programming?Deep learning frameworks (e.g., PyTorch, TensorFlow, MxNet) enable defining a target model in a deep and composite network structure assembling the building blocks of component models (= parametric linear or non-linear functions including neural nets) that run in data-dependent, procedural and conditional manner. Each component may be based on different sets of feature variables. Also, they provide a tool to estimate the model parameters in terms of differentiable optimization like stochastic gradient decent (SGD) and back-propagation algorithms.Yann LeCun earlier made good points on differentiable programming.  “Differentiable Programming is little more than a rebranding of the modern collection Deep Learning techniques, the same way Deep Learning was a rebranding of the modern incarnations of neural nets with more than two layers. But the important point is that people are now building a new kind of software by assembling networks of parameterized functional blocks and by training them from examples using some form of gradient-based optimization. An increasingly large number of people are defining the network procedurally in a data-dependant way (with loops and conditionals), allowing them to change dynamically as a function of the input data fed to them. It’s really very much like a regular progam, except it’s parameterized, automatically differentiated, and trainable/optimizable. Dynamic networks have become increasingly popular (particularly for NLP), thanks to deep learning frameworks that can handle them such as PyTorch and Chainer (note: our old deep learning framework Lush could handle a particular kind of dynamic nets called Graph Transformer Networks, back in 1994. It was needed for text recognition).”Since Bayesian modeling is based on a probabilistic model of the generative process relating the observed data with the uncertain latent variables (= generating parameters), it is very desirable to have the representational power of a deep and composite network model to sufficiently describe the potentially complex generative processes with multiple input variables. In addition, the exact calculation of the posterior distribution of latent variables requires doing the integral calculation to obtain the evidence of the observed data with the assumed prior distribtuion, so this is intractable in most problems. Thus, we need approximate Bayesian inference techniques, such as variational inference (VI). Thankfully, VI transforms the approximate posterior inference problems into the optimization problems searching for the best hyperparameters of approximate posterior distribution (often assumed to be Gaussian distributions). We will discuss it in detail below.Why Bayesian Inference?Bayesian probabilistic modeling provides a unified scheme on how to update the uncertain information (or infer the posterior distributions) about modeling parameters or latent variables using observed data. It also assumes the specification of generative processes (or model functions describing how outputs are produced from inputs) and prior distributions of modeling parameters or latent variables. This specification allows for easy incorporation of prior knowledge into the model form and associated parameter uncertainty.Although the initial choice of compared models and associated prior distributions may depend on our domain knowledge about the underlying problems, bayesian reasoning provides an objective scheme to compare different models and priors.Bayesian modeling allows us to build more robust and less overfitted models under uncertainty and predict probabilistic estimates about target variables in the model. By incorporating the known form of a physics-based equation describing the potental causal relationships of variables in the underlying phenomenon into our modeling and set the priors on the model parameters, we can build a heuristically reasonable, more generalizable and updatable model with insufficient data.It sounds all good and simple, but a key difficulty in Bayesian probabilistic modeling arises from calculating the posterior distributions for a given complicated model structure and prior. It is very often intractable to compute the “exact” posterior distribution, but the variational inference (VI), one of the important methods in deep probabilistic programming, present a commonly-applicable approach to compute the “approximate” bayesian posterior.Since VI transforms Bayesian posterior inference problems (i.e., learning uncertain modeling parameters or latent variables) into optimization problems, the SGD optimization in underlying deep learning frameworks can solve the posterior inference problems.Deep Probabilistic Programming = Bayesian Differentiable ProgrammingCompared to non-probabilistic deep learning frameworks 2 that aim to make “deterministic” models for point-estimate predictions, deep probabilistic programming frameworks enables us to 1) specify “probabilistic” models involving the uncertain distributions of model parameters or latent variables, 2) provide approximate Bayesian inferences (e.g., variational inferences) using the powerful stochastic gradient decent algorithms of the original deep learning. Thus, deep probabilistic programming naturally integrates the benefits of bayesian modeling and deep learning.Application: Bayesian Regression with a Parametric Function (e.g., knowledge-based known function form, NNs)In Bayesian modeling we posit that the model parameters (a.k.a. latent or hidden variables) generating the observed data are uncertain in our knowledge. Thus, our information about the true values of generating variables is described by a probability. That is, we use a probability to denote our uncertainty about the hidden variables selected to describe the generating process.Suppose we have a dataset  where each data point has feature input vector  and observed output variable  ​The goal of Bayesian regression is to fit a function to the data:  assuming that  is the uncertain latent variables described by a probability distribution.There are important modeling assumptions here.   is an assumed generating function we specify with unexplained error .   is a deterministic function for any sampled value of . The function  may be any known form of an equation or a neural network involving the model parameters .  The level of  is assumed to be fixed as a constant value and also related to how accurately we may specify our function .Whereas a non-Bayesian (deterministic) approach views  as a fixed variable to be estimated, a Bayesian (probabilistic) approach regards  as an uncertain variable whose probability distribution is to be estimated to explain the observed data. Maximum likelihood (ML) or maximum a posteriori (MAP) estimations are well-known non-Bayesian approaches determining a fixed .Now let’s represent the above Bayesian regression in terms of probability distributions.In the Bayesian perspective the complete generative process should be always described in the joint probability distribution of all observed and latent variables.Since  is given and  and  are known and  fixed, the joint distribution for the complete generative process is  where  denotes the hypothesis space of model form .Factorizing , we represent the complete generative process in the combination of the likelihood and the prior distributions. It is important to note that the exact forms of the likelihood and the prior distributions are part of our modeling assumptions.  The likelihood  is our assumed probability model to describe a generating process of the observed variable  from a sample of latent variables . Assuming that the likelihood is normally distributed with  as  (= the expected value of ) and  as the Gaussian noise level,Note that a known deterministic physical model  can be easily incorporated into the likelihood.  The prior  is our assumed probability model to represent the uncertain information of latent variables  (= model parameters) before we consider the observed data ..Bayesian posterior inference is to update our probabilistic information about  after observing the data  and considering the likelihood.  Mathematically the Bayes’ rule (a.k.a. inverse probability) provides this update rule and the posterior distribution representing the uncertainty about  is computed as = .The evidence  is the normalizing constant of the posterior distribution and should be calculated to obtain the exact posterior distribution 3. It is the probability that the model observes the data with the assumed likelihood and the prior distributions. Note that it is also the marginalized probability of likelihood  prior over the uncertain parameters .Since this is intractable to be exactly calculated, we involve an approximate Bayesian inference such as variational inference (VI) or Markov chain Monte Carlo (MCMC).Importantly, the Bayesian prediction of  for a new  is essentially the marginalization of likelihood  posterior over the uncertain parameters , incorporating the posterior uncertainty about parameters  into predictions.Variational InferenceFor simpler notations, we define the generating parameters . Note that  posits the likelihood variance (or unexplained error level) and  posits the prior variance (or model parameter uncertainty).  Now we can denotethe prior  by  and the posteror  by .The prior distribution: The posterior distribution:  = The aim of variational inference is to approximate the posterior  by a simpler probability distribution  where  is called the variational parameters.Therefore,Note that the log evidence  is lower-bounded because  is always greater than 0. The lower bound  is called the Evidence Lower Bound (ELBO) or the variational free energy. Since  is fixed for a given  (prior variance  and likelihood variance ) and the  hypothesis space  of the model function form , the maximization of ELBO by adjusting the variational parameters  (or ) minimizes , which makes the approximate posterior distribution  closer to the true posterior distribution . Also, note that when ELBO is maximized up to the log evidence ,  becomes 0, making the approxmiate posterior  the same as the exact posterior .The existence of ELBO transforms the posterior inference problem into the optmization problem, which can be solved using the minus ELBO as the loss function of SGD algorithm in DL frameworks. In addition, ELBO can be represented in a different way to clarify how the approximate posterior distribution of parameters  shaped by  reckons the balance between the likelihood and the prior.The first term  motivates the distribution  to concentrate on the  values with which the model have higher likelihood , whereas the second term the minus  inspires the approximate prior  to be less deviated from the given prior distribution .Prediction Errors as Probabilistic InterpretationsIn Bayesian approach we make our assumptions on both likelihood and prior distributions. The likelihood distribution posits the model function form  and the unexplained error level . The prior distribution has an assumed variance .Importantly, the Bayesian prediction of  for a new  is essentially the marginalization of likelihood  posterior over the uncertain parameters , incorporating the posterior uncertainty about parameters  into predictions.First, Note that the unexplained error  =  =  = , since  where  is the hidden true value (before a noise is being added) and  is the inherent noise. Also,  is the misspecified model bias error,  and .Thus, the likelihood for a given  is shaped by both the misspecified model bias error and the inherent error. As the ratio of the ratio  goes up, the likelihood goes down. In addition, the normalizing factor  decreases the likelihood with higher .Second, the term  where  put different weights over  values. That is,  closer to the current prior mean  is more weighted, allowing for the tendency toward a smaller deviation from the current prior. In particular, when ,  corresponds to a regularization term preferring small values of  decreasing the tendency of overfitting and higher model parameter uncertainty (= model variance error).In general, when the set of features we can use in modeling are:  only the essential features          we tend to obtain a generalizable model with a good fit        all essential features + many irrelevant features          too complicated model (overfit) and high model variance (model parameter uncertainty)        insufficient essential features          too simple model (underfit) and high model bias (model misspecification)        insufficient essential features + many irrelevant features          non-generalizable model with a bad fit                          Learning the probability distributions of model parameters or latent variables is called inference in probabilistic modeling. Bayesian inference often involves approximating posterior distributions, since the exact calculations are often intractable. In parameterized models, variational inference (VI) approximate the posterior distributions through optimization (differential programming). &#8617;              Non-probabilistic deep learning models do not assume the uncertainty in model parameters (e.g., weights in NN). &#8617;              Compared to the Bayesian approach requiring the estimation of the evidence and the posterior distribution, the maximum a posteriori (MAP) estimation simply considers the point parameter estimate  maximizing the numerator part of the posterior (= Likelihood  Prior).  &#8617;      ",
        "url": "//2018/01/15/deep-prob-modeling.html"
      }
      
    
  };
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/lunr.js/0.7.1/lunr.min.js"></script>
<script src="/assets/js/search.js"></script>
</section>
</article>

    </div>
    
<script src="/assets/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text"></p>
</footer>


  </body>
</html>
